{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8455250,"sourceType":"datasetVersion","datasetId":5039237}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-22T14:57:22.400830Z","iopub.execute_input":"2024-05-22T14:57:22.401572Z","iopub.status.idle":"2024-05-22T14:57:22.418966Z","shell.execute_reply.started":"2024-05-22T14:57:22.401539Z","shell.execute_reply":"2024-05-22T14:57:22.418045Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/bevel-mfs-coupled-fs2048-27x-27y-27z-eo-all-mat/bevel_mfs_coupled_fs2048_27x_27y_27z_eo_all.mat\n","output_type":"stream"}]},{"cell_type":"code","source":"import scipy.io\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:57:25.720547Z","iopub.execute_input":"2024-05-22T14:57:25.721214Z","iopub.status.idle":"2024-05-22T14:57:25.778019Z","shell.execute_reply.started":"2024-05-22T14:57:25.721176Z","shell.execute_reply":"2024-05-22T14:57:25.777281Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"file_path = \"/kaggle/input/bevel-mfs-coupled-fs2048-27x-27y-27z-eo-all-mat/bevel_mfs_coupled_fs2048_27x_27y_27z_eo_all.mat\"\ndf= scipy.io.loadmat(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:57:25.960182Z","iopub.execute_input":"2024-05-22T14:57:25.961072Z","iopub.status.idle":"2024-05-22T14:57:29.445467Z","shell.execute_reply.started":"2024-05-22T14:57:25.961040Z","shell.execute_reply":"2024-05-22T14:57:29.444631Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:26:06.313589Z","iopub.execute_input":"2024-05-22T14:26:06.313942Z","iopub.status.idle":"2024-05-22T14:26:06.326430Z","shell.execute_reply.started":"2024-05-22T14:26:06.313912Z","shell.execute_reply":"2024-05-22T14:26:06.325480Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Sun Jan 14 23:06:30 2024',\n '__version__': '1.0',\n '__globals__': [],\n 'bg_mfs_27x_27y_27z_30s': array([[  3.60406  ,  -0.0694835,   1.96953  , ...,  -4.92852  ,\n          -1.06204  ,  -3.15336  ],\n        [  1.1152   ,   0.954336 ,  -0.412014 , ..., -14.9024   ,\n           2.3964   ,  -0.928266 ],\n        [ -0.963937 ,   0.773058 ,  -0.46902  , ...,   2.70827  ,\n          -0.825266 ,   2.53328  ],\n        ...,\n        [  2.72128  ,  -0.490284 ,   1.11379  , ...,   3.01822  ,\n          -0.124835 ,  -1.65456  ],\n        [  1.88743  ,  -1.04309  ,   0.166072 , ...,  -0.634442 ,\n           1.33778  ,  -3.89464  ],\n        [ -0.0363668,   0.0581092,  -0.501747 , ...,   3.37628  ,\n           1.04348  ,   4.83966  ]]),\n 'bg_mfs_27x_27y_27z_30s_ceeo': array([[ 11.36188306,   0.56339061,  -1.48545005, ...,  22.26257277,\n          -1.85670185,   5.64019801],\n        [ 11.36188306,   0.56339061,  -1.48545005, ...,  22.26257277,\n          -1.85670185,   5.64019801],\n        [ 11.36188306,   0.56339061,  -1.48545005, ...,  22.26257277,\n          -1.85670185,   5.64019801],\n        ...,\n        [  7.25808003,   0.21206655,   1.23926303, ..., -14.60148979,\n           2.26870753,  -5.76818205],\n        [  7.25808003,   0.21206655,   1.23926303, ..., -14.60148979,\n           2.26870753,  -5.76818205],\n        [  7.25808003,   0.21206655,   1.23926303, ..., -14.60148979,\n           2.26870753,  -5.76818205]]),\n 'bg_mfs_27x_27y_27z_30s_eo': array([[ 4.71775782e+00,  9.64471976e-01,  1.09350450e+00, ...,\n          2.35429289e+02,  4.86626746e+00,  8.85002159e+00],\n        [ 4.71775782e+00,  9.64471976e-01,  1.09350450e+00, ...,\n          2.35429289e+02,  4.86626746e+00,  8.85002159e+00],\n        [ 2.75351907e+00,  2.89164883e-02, -6.84960313e-03, ...,\n          8.20419478e+01, -1.09562139e+01,  4.11965934e+00],\n        ...,\n        [ 8.83587635e+00,  1.30632816e-01,  1.14213482e+00, ...,\n          1.16772451e+01, -1.25706640e+00, -7.66910402e+00],\n        [ 1.88743000e+00, -1.04309000e+00,  1.66072000e-01, ...,\n         -6.34442000e-01,  1.33778000e+00, -3.89464000e+00],\n        [ 1.88743000e+00, -1.04309000e+00,  1.66072000e-01, ...,\n         -6.34442000e-01,  1.33778000e+00, -3.89464000e+00]]),\n 'bg_mfs_27x_27y_27z_30s_eo123': array([[  3.60406   ,  -0.0694835 ,   1.96953   , ...,  -4.92852   ,\n          -1.06204   ,  -3.15336   ],\n        [  1.1152    ,   0.954336  ,  -0.412014  , ..., -14.9024    ,\n           2.3964    ,  -0.928266  ],\n        [ -0.08519808,   0.77323767,   0.58771549, ...,  -8.56392769,\n          25.10754935,   5.84895949],\n        ...,\n        [  2.72128   ,  -0.490284  ,   1.11379   , ...,   3.01822   ,\n          -0.124835  ,  -1.65456   ],\n        [  1.88743   ,  -1.04309   ,   0.166072  , ...,  -0.634442  ,\n           1.33778   ,  -3.89464   ],\n        [ -0.0363668 ,   0.0581092 ,  -0.501747  , ...,   3.37628   ,\n           1.04348   ,   4.83966   ]]),\n 'bg_mfs_27x_27y_27z_30s_eo23': array([[  3.60406   ,  -0.0694835 ,   1.96953   , ...,  -4.92852   ,\n          -1.06204   ,  -3.15336   ],\n        [  1.1152    ,   0.954336  ,  -0.412014  , ..., -14.9024    ,\n           2.3964    ,  -0.928266  ],\n        [  0.0289839 ,   0.03731039,  -0.1215043 , ..., -25.49210155,\n           3.49734614,  -0.90320328],\n        ...,\n        [  2.72128   ,  -0.490284  ,   1.11379   , ...,   3.01822   ,\n          -0.124835  ,  -1.65456   ],\n        [  1.88743   ,  -1.04309   ,   0.166072  , ...,  -0.634442  ,\n           1.33778   ,  -3.89464   ],\n        [ -0.0363668 ,   0.0581092 ,  -0.501747  , ...,   3.37628   ,\n           1.04348   ,   4.83966   ]]),\n 'label_mfs': array([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [2],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3],\n        [3]], dtype=uint8)}"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:32:15.601795Z","iopub.execute_input":"2024-05-22T13:32:15.602147Z","iopub.status.idle":"2024-05-22T13:32:15.630186Z","shell.execute_reply.started":"2024-05-22T13:32:15.602117Z","shell.execute_reply":"2024-05-22T13:32:15.628861Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_temp\u001b[49m[\u001b[38;5;241m70\u001b[39m]  \u001b[38;5;66;03m# Assuming X_train_temp is a list of image paths\u001b[39;00m\n\u001b[1;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n","\u001b[0;31mNameError\u001b[0m: name 'X_train_temp' is not defined"],"ename":"NameError","evalue":"name 'X_train_temp' is not defined","output_type":"error"}]},{"cell_type":"code","source":"data=pd.DataFrame(df['bg_mfs_27x_27y_27z_30s_eo123'])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:57:46.280967Z","iopub.execute_input":"2024-05-22T14:57:46.281837Z","iopub.status.idle":"2024-05-22T14:57:46.286526Z","shell.execute_reply.started":"2024-05-22T14:57:46.281796Z","shell.execute_reply":"2024-05-22T14:57:46.285472Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import spectrogram\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom PIL import Image\n\n# Define segment size and step size\nsegment_size = 2048\nstep_size = 1024\n\n# Function to generate and save spectrogram image\ndef generate_spectrogram_image(segment_signal, label, segment_id, output_dir, image_size=(100, 100)):\n    # Sampling frequency\n    fs = 2048  # 2048 samples per second\n    \n    # Generate the spectrogram\n    frequencies, times, Sxx = spectrogram(segment_signal, fs)\n    \n    # Plot the spectrogram\n    plt.figure(figsize=(5, 5))\n    plt.axis('off')\n    plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx), shading='gouraud')\n    \n    # Save the spectrogram image temporarily\n    temp_image_path = 'temp_spectrogram.png'\n    plt.savefig(temp_image_path, bbox_inches='tight', pad_inches=0)  # Save the image\n    plt.close()  # Close the plot to release memory\n    \n    # Open the saved image and resize it\n    img = Image.open(temp_image_path)\n    img_resized = img.resize(image_size, Image.ANTIALIAS)\n    \n    # Define the final image path and save the resized image\n    image_filename = f\"spectrogram_label_{label}_segment_{segment_id}.png\"\n    image_path = os.path.join(output_dir, image_filename)\n    img_resized.save(image_path)\n    \n    # Remove the temporary image\n    os.remove(temp_image_path)\n    \n    return image_path\n\n# Initialize lists to store spectrogram images and labels\nspectrogram_images = []\nlabels = []\n# Create an output directory for the images\noutput_dir = 'spectrogram_images'\nos.makedirs(output_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:57:37.521460Z","iopub.execute_input":"2024-05-22T14:57:37.521854Z","iopub.status.idle":"2024-05-22T14:57:37.578995Z","shell.execute_reply.started":"2024-05-22T14:57:37.521824Z","shell.execute_reply":"2024-05-22T14:57:37.578248Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Loop over each column in the cooe DataFrame\nfor i in range(data.shape[1]):\n    # Assign label based on the current iteration\n    label = (i // 27) + 1  # Divide by 27 and add 1 to get labels 1, 2, 3 for each group of 27\n    \n    # Extract signal data from the current column\n    signal_data = np.array(data[i])\n    \n    # Extract segments from the signal data\n    for start in range(0, len(signal_data) - segment_size + 1, step_size):\n        segment_signal = signal_data[start:start + segment_size]\n        segment_signal = segment_signal.flatten()  # Flatten the segment\n        \n        # Generate and save spectrogram image\n        image_path = generate_spectrogram_image(segment_signal, label, start, output_dir)\n        \n        # Append the image path and label to the respective lists\n        spectrogram_images.append(image_path)\n        labels.append(label)\n\n\nfrom sklearn.preprocessing import LabelEncoder \nimport tensorflow \nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn import preprocessing\nY=np.array(labels)\n\nencoder=LabelEncoder()\nencoder.fit(labels)\nencoded_Y=encoder.transform(labels)\nOHE_Y=to_categorical(encoded_Y)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:58:07.334074Z","iopub.execute_input":"2024-05-22T14:58:07.334487Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_938/587389018.py:32: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n  img_resized = img.resize(image_size, Image.ANTIALIAS)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.20,shuffle=True)\n\n# Split the temporary data into validation and testing data (50% for each)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50,shuffle=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom sklearn.model_selection import train_test_split\ninput_shape = (100, 100, 3)  # Assuming RGB images of size 100x100\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.20,shuffle=True)\n\n# Split the temporary data into validation and testing data (50% for each)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50,shuffle=True)\n\n\n# Load and preprocess images for training data\nX_train_temp_images = []\nfor img_path in X_train_temp:\n    img = load_img(img_path, target_size=input_shape[:2])\n    img_array = img_to_array(img)\n    X_train_temp_images.append(img_array)\n\n# Convert lists to arrays for training data\nX_train_temp_images = np.array(X_train_temp_images)\ny_train_temp = np.array(y_train_temp)\n\n# Load and preprocess images for validation data\nX_valid_images = []\nfor img_path in X_valid:\n    img = load_img(img_path, target_size=input_shape[:2])\n    img_array = img_to_array(img)\n    X_valid_images.append(img_array)\n\n# Convert lists to arrays for validation data\nX_valid_images = np.array(X_valid_images)\ny_valid = np.array(y_valid)\n\n# Load and preprocess images for test data\nX_test_images = []\nfor img_path in X_test:\n    img = load_img(img_path, target_size=input_shape[:2])\n    img_array = img_to_array(img)\n    X_test_images.append(img_array)\n\n# Convert lists to arrays for test data\nX_test_images = np.array(X_test_images)\ny_test = np.array(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:42:44.538444Z","iopub.execute_input":"2024-05-22T14:42:44.538784Z","iopub.status.idle":"2024-05-22T14:42:47.964793Z","shell.execute_reply.started":"2024-05-22T14:42:44.538753Z","shell.execute_reply":"2024-05-22T14:42:47.963998Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train_temp_images.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:48:43.565329Z","iopub.execute_input":"2024-05-22T13:48:43.565634Z","iopub.status.idle":"2024-05-22T13:48:43.571791Z","shell.execute_reply.started":"2024-05-22T13:48:43.565591Z","shell.execute_reply":"2024-05-22T13:48:43.570846Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(3823, 100, 100, 3)"},"metadata":{}}]},{"cell_type":"code","source":"y_train_temp.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:08:43.245052Z","iopub.execute_input":"2024-05-22T11:08:43.245364Z","iopub.status.idle":"2024-05-22T11:08:43.254858Z","shell.execute_reply.started":"2024-05-22T11:08:43.245333Z","shell.execute_reply":"2024-05-22T11:08:43.253921Z"},"trusted":true},"execution_count":190,"outputs":[{"execution_count":190,"output_type":"execute_result","data":{"text/plain":"(3823, 3)"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Define your spectrogram images and labels\n# X_train_temp, X_valid, X_test are lists of image paths\n# y_train_temp, y_valid, y_test are numpy arrays of labels\n\n# Define input shape\n\n# Number of classes\nnum_classes = 3\n\n# Define the model\nmodel = Sequential()\n\n# Add convolutional layers\nmodel.add(Conv2D(128, (3, 3), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\n# Flatten the output for fully connected layers\nmodel.add(Flatten())\n\n# Add dropout layer\nmodel.add(Dropout(0.2))\n\n# Add fully connected layers\nmodel.add(Dense(64, activation='relu'))\n\n# Output layer\nmodel.add(Dense(num_classes, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n\nhistory = model.fit(X_train_temp_images, y_train_temp, batch_size=32, epochs=150, \n                    validation_data=(X_valid_images, y_valid), callbacks=[early_stop])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:08:43.255951Z","iopub.execute_input":"2024-05-22T11:08:43.256272Z","iopub.status.idle":"2024-05-22T11:09:24.030283Z","shell.execute_reply.started":"2024-05-22T11:08:43.256229Z","shell.execute_reply":"2024-05-22T11:09:24.029423Z"},"trusted":true},"execution_count":191,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m3,584\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_28 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m73,792\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_29 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_30 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m9,248\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_31 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m32,832\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,115\u001b[0m (539.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,115</span> (539.51 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m138,115\u001b[0m (539.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,115</span> (539.51 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.5510 - loss: 10.7182 - val_accuracy: 0.9561 - val_loss: 0.4635\nEpoch 2/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8588 - loss: 2.6276 - val_accuracy: 0.9477 - val_loss: 0.8109\nEpoch 3/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9282 - loss: 3.3973 - val_accuracy: 0.9979 - val_loss: 0.0160\nEpoch 4/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9585 - loss: 3.9776 - val_accuracy: 0.9561 - val_loss: 3.5309\nEpoch 5/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9502 - loss: 19.6152 - val_accuracy: 0.9498 - val_loss: 12.7977\nEpoch 6/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9671 - loss: 7.8219 - val_accuracy: 0.9728 - val_loss: 5.5399\nEpoch 7/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9726 - loss: 11.8971 - val_accuracy: 0.9854 - val_loss: 5.6228\nEpoch 8/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9353 - loss: 123.1333 - val_accuracy: 0.9854 - val_loss: 24.0926\nEpoch 9/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9790 - loss: 34.6203 - val_accuracy: 0.9895 - val_loss: 4.7678\nEpoch 10/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9826 - loss: 44.7257 - val_accuracy: 0.9833 - val_loss: 22.3615\nEpoch 11/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9800 - loss: 23.7679 - val_accuracy: 0.9916 - val_loss: 3.0514\nEpoch 12/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9964 - loss: 3.0005 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 13/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9989 - loss: 0.5012 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 14/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9978 - loss: 1.4350 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 15/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9982 - loss: 0.6573 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 16/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9980 - loss: 0.8927 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 17/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9982 - loss: 1.2434 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 18/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9969 - loss: 3.1668 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 19/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9980 - loss: 0.9091 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 20/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9951 - loss: 2.8593 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 21/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9974 - loss: 1.9149 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 22/150\n\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9975 - loss: 2.6525 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\nEpoch 22: early stopping\nRestoring model weights from the end of the best epoch: 12.\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:09:24.031981Z","iopub.execute_input":"2024-05-22T11:09:24.032268Z","iopub.status.idle":"2024-05-22T11:09:24.340474Z","shell.execute_reply.started":"2024-05-22T11:09:24.032244Z","shell.execute_reply":"2024-05-22T11:09:24.339635Z"},"trusted":true},"execution_count":192,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import accuracy_score, confusion_matrix\ndef inv_transform_result(y_pred):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred)\n    return y_pred\n    \n    \n    \ny_pred=model.predict(X_test_images)\n\n\nY_pred=inv_transform_result(y_pred)\nY_test=inv_transform_result(y_test)\n\nfrom sklearn.decomposition import PCA\n# Calculate accuracy\naccuracy = accuracy_score(Y_test, Y_pred)\nprint(accuracy)\n\n# Calculate confusion matrix\nconfusion_mat = confusion_matrix(Y_test, Y_pred)\n\n# Convert confusion matrix into percentages\nconfusion_mat_percent = confusion_mat.astype('float') / confusion_mat.sum(axis=1)[:, np.newaxis] * 100\n\n# Visualize confusion matrix using heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_mat_percent, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False)\nplt.title(\"Confusion Matrix (Percentage)\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:09:24.341604Z","iopub.execute_input":"2024-05-22T11:09:24.341901Z","iopub.status.idle":"2024-05-22T11:09:25.305009Z","shell.execute_reply.started":"2024-05-22T11:09:24.341875Z","shell.execute_reply":"2024-05-22T11:09:25.304146Z"},"trusted":true},"execution_count":193,"outputs":[{"name":"stdout","text":"\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n1.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred)\n    return y_pred\n\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    # Ensure the lengths of spectrogram_images and OHE_Y are consistent\n    if len(spectrogram_images) != len(OHE_Y):\n        raise ValueError(f\"Inconsistent number of samples: {len(spectrogram_images)} images, {len(OHE_Y)} labels\")\n\n    for i in range(num_iterations):\n        # Split the dataset into training and temporary data (80% for training, 20% for temporary data)\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.20, shuffle=True)\n\n        # Split the temporary data into validation and testing data (50% for each)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        # Load and preprocess images for test data\n        X_test_images = []\n        for img_path in X_test:\n            img = load_img(img_path, target_size=input_shape[:2])\n            img_array = img_to_array(img)\n            X_test_images.append(img_array)\n\n        # Convert lists to arrays for test data\n        X_test_images = np.array(X_test_images)\n        y_test = np.array(y_test)\n        \n        # Predict\n        y_pred = model.predict(X_test_images)\n        \n        # Inverse transform\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        # Calculate metrics\n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    # Calculate mean and standard deviation for metrics\n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    mean_recall = np.mean(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_precision = np.std(precisions)\n    std_dev_recall = np.std(recalls)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: model, spectrogram_images, OHE_Y, encoder, input_shape\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:09:25.308235Z","iopub.execute_input":"2024-05-22T11:09:25.308628Z","iopub.status.idle":"2024-05-22T11:09:30.660630Z","shell.execute_reply.started":"2024-05-22T11:09:25.308602Z","shell.execute_reply":"2024-05-22T11:09:30.659714Z"},"trusted":true},"execution_count":194,"outputs":[{"name":"stdout","text":"\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \nMean Accuracy: 1.0\nStandard Deviation of Accuracy: 0.0\nMean Precision: 1.0\nStandard Deviation of Precision: 0.0\nMean Recall: 1.0\nStandard Deviation of Recall: 0.0\nMean F1 Score: 1.0\nStandard Deviation of F1 Score: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_temp_images.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:09:30.661754Z","iopub.execute_input":"2024-05-22T11:09:30.662051Z","iopub.status.idle":"2024-05-22T11:09:30.668963Z","shell.execute_reply.started":"2024-05-22T11:09:30.662025Z","shell.execute_reply":"2024-05-22T11:09:30.667876Z"},"trusted":true},"execution_count":195,"outputs":[{"execution_count":195,"output_type":"execute_result","data":{"text/plain":"(3823, 100, 100, 3)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred)\n    return y_pred\n\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    # Ensure the lengths of spectrogram_images and OHE_Y are consistent\n    if len(spectrogram_images) != len(OHE_Y):\n        raise ValueError(f\"Inconsistent number of samples: {len(spectrogram_images)} images, {len(OHE_Y)} labels\")\n\n    for i in range(num_iterations):\n        # Split the dataset into training and temporary data (80% for training, 20% for temporary data)\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.30, shuffle=True)\n\n        # Split the temporary data into validation and testing data (50% for each)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        # Load and preprocess images for test data\n        X_test_images = []\n        for img_path in X_test:\n            img = load_img(img_path, target_size=input_shape[:2])\n            img_array = img_to_array(img)\n            X_test_images.append(img_array)\n\n        # Convert lists to arrays for test data\n        X_test_images = np.array(X_test_images)\n        y_test = np.array(y_test)\n        \n        # Predict\n        y_pred = model.predict(X_test_images)\n        \n        # Inverse transform\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        # Calculate metrics\n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    # Calculate mean and standard deviation for metrics\n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    mean_recall = np.mean(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_precision = np.std(precisions)\n    std_dev_recall = np.std(recalls)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: model, spectrogram_images, OHE_Y, encoder, input_shape\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:09:30.670446Z","iopub.execute_input":"2024-05-22T11:09:30.670793Z","iopub.status.idle":"2024-05-22T11:09:39.944597Z","shell.execute_reply.started":"2024-05-22T11:09:30.670767Z","shell.execute_reply":"2024-05-22T11:09:39.943633Z"},"trusted":true},"execution_count":196,"outputs":[{"name":"stdout","text":"\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\nMean Accuracy: 0.998744769874477\nStandard Deviation of Accuracy: 0.0013157574800636853\nMean Precision: 0.9987550262896203\nStandard Deviation of Precision: 0.0012984807396408387\nMean Recall: 0.998735057047902\nStandard Deviation of Recall: 0.0013253630460555417\nMean F1 Score: 0.9987415955021645\nStandard Deviation of F1 Score: 0.0013151568198978038\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred)\n    return y_pred\n\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    # Ensure the lengths of spectrogram_images and OHE_Y are consistent\n    if len(spectrogram_images) != len(OHE_Y):\n        raise ValueError(f\"Inconsistent number of samples: {len(spectrogram_images)} images, {len(OHE_Y)} labels\")\n\n    for i in range(num_iterations):\n        # Split the dataset into training and temporary data (80% for training, 20% for temporary data)\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.40, shuffle=True)\n\n        # Split the temporary data into validation and testing data (50% for each)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        # Load and preprocess images for test data\n        X_test_images = []\n        for img_path in X_test:\n            img = load_img(img_path, target_size=input_shape[:2])\n            img_array = img_to_array(img)\n            X_test_images.append(img_array)\n\n        # Convert lists to arrays for test data\n        X_test_images = np.array(X_test_images)\n        y_test = np.array(y_test)\n        \n        # Predict\n        y_pred = model.predict(X_test_images)\n        \n        # Inverse transform\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        # Calculate metrics\n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    # Calculate mean and standard deviation for metrics\n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    mean_recall = np.mean(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_precision = np.std(precisions)\n    std_dev_recall = np.std(recalls)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: model, spectrogram_images, OHE_Y, encoder, input_shape\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T11:09:39.946327Z","iopub.execute_input":"2024-05-22T11:09:39.946713Z","iopub.status.idle":"2024-05-22T11:09:51.638157Z","shell.execute_reply.started":"2024-05-22T11:09:39.946685Z","shell.execute_reply":"2024-05-22T11:09:51.637177Z"},"trusted":true},"execution_count":197,"outputs":[{"name":"stdout","text":"\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\nMean Accuracy: 0.9972803347280335\nStandard Deviation of Accuracy: 0.001494022683795578\nMean Precision: 0.9973138629983817\nStandard Deviation of Precision: 0.0014316428874114654\nMean Recall: 0.9972829866672225\nStandard Deviation of Recall: 0.0015387110806844538\nMean F1 Score: 0.997288413170876\nStandard Deviation of F1 Score: 0.0014900037301819279\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size):\n    images = []\n    for img_path in image_paths:\n        img = load_img(img_path, target_size=target_size)\n        img_array = img_to_array(img)\n        img_array = preprocess_input(img_array)  # Preprocess for ResNet\n        images.append(img_array)\n    return np.array(images)\n\n# Function to inverse transform results\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred.reshape(-1, 1))\n    return y_pred\n\n# Load pre-trained ResNet50 model + higher level layers\ndef create_resnet_model(input_shape, num_classes):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = base_model.output\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # Freeze the layers of the base model\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Function to unfreeze the last few layers of the base model for fine-tuning\ndef unfreeze_last_layers(model, num_layers):\n    for layer in model.layers[-num_layers:]:\n        if not isinstance(layer, Dense):\n            layer.trainable = True\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Function to predict and evaluate\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for _ in range(num_iterations):\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.20, shuffle=True)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        X_test_images = load_and_preprocess_images(X_test, target_size=input_shape[:2])\n        y_test = np.array(y_test)\n        \n        y_pred = model.predict(X_test_images)\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    std_dev_precision = np.std(precisions)\n    mean_recall = np.mean(recalls)\n    std_dev_recall = np.std(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: spectrogram_images, OHE_Y, encoder\ninput_shape = (224, 224, 3)  # ResNet input shape\nnum_classes = 3  # Assuming 3 classes\n\n# Convert OHE_Y to numpy array if it isn't already\nif not isinstance(OHE_Y, np.ndarray):\n    OHE_Y = np.array(OHE_Y)\n\nmodel = create_resnet_model(input_shape, num_classes)\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n\n# Train the model\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.20, shuffle=True)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n\nX_train_temp_images = load_and_preprocess_images(X_train_temp, target_size=input_shape[:2])\nX_valid_images = load_and_preprocess_images(X_valid, target_size=input_shape[:2])\n\n# Initial training of the top layers\nmodel.fit(X_train_temp_images, y_train_temp, batch_size=32, epochs=10, validation_data=(X_valid_images, y_valid), callbacks=[early_stop])\n\n\n# Perform predictions and evaluation\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size):\n    images = []\n    for img_path in image_paths:\n        img = load_img(img_path, target_size=target_size)\n        img_array = img_to_array(img)\n        img_array = preprocess_input(img_array)  # Preprocess for ResNet\n        images.append(img_array)\n    return np.array(images)\n\n# Function to inverse transform results\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred.reshape(-1, 1))\n    return y_pred\n\n# Load pre-trained ResNet50 model + higher level layers\ndef create_resnet_model(input_shape, num_classes):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = base_model.output\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # Freeze the layers of the base model\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Function to unfreeze the last few layers of the base model for fine-tuning\ndef unfreeze_last_layers(model, num_layers):\n    for layer in model.layers[-num_layers:]:\n        if not isinstance(layer, Dense):\n            layer.trainable = True\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Function to predict and evaluate\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for _ in range(num_iterations):\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.30, shuffle=True)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        X_test_images = load_and_preprocess_images(X_test, target_size=input_shape[:2])\n        y_test = np.array(y_test)\n        \n        y_pred = model.predict(X_test_images)\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    std_dev_precision = np.std(precisions)\n    mean_recall = np.mean(recalls)\n    std_dev_recall = np.std(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: spectrogram_images, OHE_Y, encoder\ninput_shape = (224, 224, 3)  # ResNet input shape\nnum_classes = 3  # Assuming 3 classes\n\n# Convert OHE_Y to numpy array if it isn't already\nif not isinstance(OHE_Y, np.ndarray):\n    OHE_Y = np.array(OHE_Y)\n\nmodel = create_resnet_model(input_shape, num_classes)\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n\n# Train the model\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.30, shuffle=True)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n\nX_train_temp_images = load_and_preprocess_images(X_train_temp, target_size=input_shape[:2])\nX_valid_images = load_and_preprocess_images(X_valid, target_size=input_shape[:2])\n\n# Initial training of the top layers\nmodel.fit(X_train_temp_images, y_train_temp, batch_size=32, epochs=10, validation_data=(X_valid_images, y_valid), callbacks=[early_stop])\n\n# Unfreeze the last few layers and fine-tune the model\n\n# Perform predictions and evaluation\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size):\n    images = []\n    for img_path in image_paths:\n        img = load_img(img_path, target_size=target_size)\n        img_array = img_to_array(img)\n        img_array = preprocess_input(img_array)  # Preprocess for ResNet\n        images.append(img_array)\n    return np.array(images)\n\n# Function to inverse transform results\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred.reshape(-1, 1))\n    return y_pred\n\n# Load pre-trained ResNet50 model + higher level layers\ndef create_resnet_model(input_shape, num_classes):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = base_model.output\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # Freeze the layers of the base model\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Function to unfreeze the last few layers of the base model for fine-tuning\ndef unfreeze_last_layers(model, num_layers):\n    for layer in model.layers[-num_layers:]:\n        if not isinstance(layer, Dense):\n            layer.trainable = True\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Function to predict and evaluate\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for _ in range(num_iterations):\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.30, shuffle=True)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        X_test_images = load_and_preprocess_images(X_test, target_size=input_shape[:2])\n        y_test = np.array(y_test)\n        \n        y_pred = model.predict(X_test_images)\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    std_dev_precision = np.std(precisions)\n    mean_recall = np.mean(recalls)\n    std_dev_recall = np.std(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: spectrogram_images, OHE_Y, encoder\ninput_shape = (224, 224, 3)  # ResNet input shape\nnum_classes = 3  # Assuming 3 classes\n\n# Convert OHE_Y to numpy array if it isn't already\nif not isinstance(OHE_Y, np.ndarray):\n    OHE_Y = np.array(OHE_Y)\n\nmodel = create_resnet_model(input_shape, num_classes)\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n\n# Train the model\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.30, shuffle=True)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n\nX_train_temp_images = load_and_preprocess_images(X_train_temp, target_size=input_shape[:2])\nX_valid_images = load_and_preprocess_images(X_valid, target_size=input_shape[:2])\n\n# Initial training of the top layers\nmodel.fit(X_train_temp_images, y_train_temp, batch_size=32, epochs=10, validation_data=(X_valid_images, y_valid), callbacks=[early_stop])\n\n# Perform predictions and evaluation\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:48:43.921854Z","iopub.status.idle":"2024-05-22T13:48:43.922166Z","shell.execute_reply.started":"2024-05-22T13:48:43.922013Z","shell.execute_reply":"2024-05-22T13:48:43.922025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VGG16","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size):\n    images = []\n    for img_path in image_paths:\n        img = load_img(img_path, target_size=target_size)\n        img_array = img_to_array(img)\n        img_array = preprocess_input(img_array)  # Preprocess for VGG16\n        images.append(img_array)\n    return np.array(images)\n\n# Function to inverse transform results\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred.reshape(-1, 1))\n    return y_pred\n\n# Load pre-trained VGG16 model + higher level layers\ndef create_vgg16_model(input_shape, num_classes):\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = base_model.output\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # Freeze the layers of the base model\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Function to unfreeze the last few layers of the base model for fine-tuning\ndef unfreeze_last_layers(model, num_layers):\n    for layer in model.layers[-num_layers:]:\n        if not isinstance(layer, Dense):\n            layer.trainable = True\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Function to predict and evaluate\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for _ in range(num_iterations):\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.20, shuffle=True)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        X_test_images = load_and_preprocess_images(X_test, target_size=input_shape[:2])\n        y_test = np.array(y_test)\n        \n        y_pred = model.predict(X_test_images)\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    std_dev_precision = np.std(precisions)\n    mean_recall = np.mean(recalls)\n    std_dev_recall = np.std(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: spectrogram_images, OHE_Y, encoder\ninput_shape = (224, 224, 3)  # VGG16 input shape\nnum_classes = 3  # Assuming 3 classes\n\n# Convert OHE_Y to numpy array if it isn't already\nif not isinstance(OHE_Y, np.ndarray):\n    OHE_Y = np.array(OHE_Y)\n\nmodel = create_vgg16_model(input_shape, num_classes)\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor=' val_accuracy', patience=10, verbose=1, restore_best_weights=True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n\n# Train the model\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.20, shuffle=True)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n\nX_train_temp_images = load_and_preprocess_images(X_train_temp, target_size=input_shape[:2])\nX_valid_images = load_and_preprocess_images(X_valid, target_size=input_shape[:2])\n\n# Initial training of the top layers\nmodel.fit(X_train_temp_images, y_train_temp, batch_size=32, epochs=10, validation_data=(X_valid_images, y_valid), callbacks=[early_stop])\n\n\n# Perform predictions and evaluation\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size):\n    images = []\n    for img_path in image_paths:\n        img = load_img(img_path, target_size=target_size)\n        img_array = img_to_array(img)\n        img_array = preprocess_input(img_array)  # Preprocess for VGG16\n        images.append(img_array)\n    return np.array(images)\n\n# Function to inverse transform results\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred.reshape(-1, 1))\n    return y_pred\n\n# Load pre-trained VGG16 model + higher level layers\ndef create_vgg16_model(input_shape, num_classes):\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = base_model.output\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # Freeze the layers of the base model\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Function to unfreeze the last few layers of the base model for fine-tuning\ndef unfreeze_last_layers(model, num_layers):\n    for layer in model.layers[-num_layers:]:\n        if not isinstance(layer, Dense):\n            layer.trainable = True\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Function to predict and evaluate\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for _ in range(num_iterations):\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.30, shuffle=True)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        X_test_images = load_and_preprocess_images(X_test, target_size=input_shape[:2])\n        y_test = np.array(y_test)\n        \n        y_pred = model.predict(X_test_images)\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    std_dev_precision = np.std(precisions)\n    mean_recall = np.mean(recalls)\n    std_dev_recall = np.std(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: spectrogram_images, OHE_Y, encoder\ninput_shape = (224, 224, 3)  # VGG16 input shape\nnum_classes = 3  # Assuming 3 classes\n\n# Convert OHE_Y to numpy array if it isn't already\nif not isinstance(OHE_Y, np.ndarray):\n    OHE_Y = np.array(OHE_Y)\n\nmodel = create_vgg16_model(input_shape, num_classes)\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor=' val_accuracy', patience=10, verbose=1, restore_best_weights=True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n\n# Train the model\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.30, shuffle=True)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n\nX_train_temp_images = load_and_preprocess_images(X_train_temp, target_size=input_shape[:2])\nX_valid_images = load_and_preprocess_images(X_valid, target_size=input_shape[:2])\n\n# Initial training of the top layers\nmodel.fit(X_train_temp_images, y_train_temp, batch_size=32, epochs=10, validation_data=(X_valid_images, y_valid), callbacks=[early_stop])\n\n\n# Perform predictions and evaluation\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size):\n    images = []\n    for img_path in image_paths:\n        img = load_img(img_path, target_size=target_size)\n        img_array = img_to_array(img)\n        img_array = preprocess_input(img_array)  # Preprocess for VGG16\n        images.append(img_array)\n    return np.array(images)\n\n# Function to inverse transform results\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred.reshape(-1, 1))\n    return y_pred\n\n# Load pre-trained VGG16 model + higher level layers\ndef create_vgg16_model(input_shape, num_classes):\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = base_model.output\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # Freeze the layers of the base model\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Function to unfreeze the last few layers of the base model for fine-tuning\ndef unfreeze_last_layers(model, num_layers):\n    for layer in model.layers[-num_layers:]:\n        if not isinstance(layer, Dense):\n            layer.trainable = True\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Function to predict and evaluate\ndef predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape, num_iterations=10):\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for _ in range(num_iterations):\n        X_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.40, shuffle=True)\n        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n        \n        X_test_images = load_and_preprocess_images(X_test, target_size=input_shape[:2])\n        y_test = np.array(y_test)\n        \n        y_pred = model.predict(X_test_images)\n        Y_pred = inv_transform_result(y_pred, encoder)\n        Y_true = inv_transform_result(y_test, encoder)\n        \n        accuracy = accuracy_score(Y_true, Y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(Y_true, Y_pred, average='macro')\n        \n        accuracies.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        f1_scores.append(f1)\n    \n    mean_accuracy = np.mean(accuracies)\n    std_dev_accuracy = np.std(accuracies)\n    mean_precision = np.mean(precisions)\n    std_dev_precision = np.std(precisions)\n    mean_recall = np.mean(recalls)\n    std_dev_recall = np.std(recalls)\n    mean_f1 = np.mean(f1_scores)\n    std_dev_f1 = np.std(f1_scores)\n    \n    return mean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1\n\n# Assume these variables are already defined: spectrogram_images, OHE_Y, encoder\ninput_shape = (224, 224, 3)  # VGG16 input shape\nnum_classes = 3  # Assuming 3 classes\n\n# Convert OHE_Y to numpy array if it isn't already\nif not isinstance(OHE_Y, np.ndarray):\n    OHE_Y = np.array(OHE_Y)\n\nmodel = create_vgg16_model(input_shape, num_classes)\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor=' val_accuracy', patience=10, verbose=1, restore_best_weights=True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n\n# Train the model\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(spectrogram_images, OHE_Y, test_size=0.40, shuffle=True)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.50, shuffle=True)\n\nX_train_temp_images = load_and_preprocess_images(X_train_temp, target_size=input_shape[:2])\nX_valid_images = load_and_preprocess_images(X_valid, target_size=input_shape[:2])\n\n\n# Fine-tuning the entire model with lower learning rate\nmodel.fit(X_train_temp_images, y_train_temp, batch_size=32, epochs=20, validation_data=(X_valid_images, y_valid), callbacks=[early_stop])\n\n# Perform predictions and evaluation\nmean_accuracy, std_dev_accuracy, mean_precision, std_dev_precision, mean_recall, std_dev_recall, mean_f1, std_dev_f1 = predict_and_evaluate(model, spectrogram_images, OHE_Y, encoder, input_shape)\n\nprint(\"Mean Accuracy:\", mean_accuracy)\nprint(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\nprint(\"Mean Precision:\", mean_precision)\nprint(\"Standard Deviation of Precision:\", std_dev_precision)\nprint(\"Mean Recall:\", mean_recall)\nprint(\"Standard Deviation of Recall:\", std_dev_recall)\nprint(\"Mean F1 Score:\", mean_f1)\nprint(\"Standard Deviation of F1 Score:\", std_dev_f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iNCEPTIONNET","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size):\n    images = []\n    for img_path in image_paths:\n        img = load_img(img_path, target_size=target_size)\n        img_array = img_to_array(img)\n        img_array = preprocess_input(img_array)  # Preprocess for InceptionV3\n        images.append(img_array)\n    return np.array(images)\n\n# Function to inverse transform results\ndef inv_transform_result(y_pred, encoder):\n    y_pred = y_pred.argmax(axis=1)\n    y_pred = encoder.inverse_transform(y_pred.reshape(-1, 1))\n    return y_pred\n\n# Load pre-trained InceptionV3 model + higher level layers\ndef create_inception_model(input_shape, num_classes):\n    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = base_model.output\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n","metadata":{},"execution_count":null,"outputs":[]}]}